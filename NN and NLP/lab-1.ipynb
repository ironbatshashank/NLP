{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_yvQBKrD8Se"
      },
      "source": [
        "# **ECS 7001 - NN & NNLP**\n",
        "\n",
        "## **2021/22 Lab 1: Skip-gram Model for Word2Vec**\n",
        "\n",
        "<br>\n",
        "\n",
        "**26th and 27th January**\n",
        "\n",
        "There are two Word2Vec architectures for creating word embeddings: the Continuous Bag of Words (CBOW) architecture and the Skip Gram architecture . In this lab, we will obtain our own word embeddings by training a skip-gram neural network model. Some of the code for this will be supplied here but in some sections, you will be required to implement the code yourself. Hints and\n",
        "tips will be provided.\n",
        "\n",
        "\n",
        "The skip gram model is essentially a feedforward neural network with one hidden layer, trained to predict the context word given a target word. There are two ways to train this model: (1) using a hierarchical softmax function and/or (2) by negative sampling. In this lab, we will be training using\n",
        "negative sampling. To train with negative sampling, the model is cast as a binary classification problem. The dataset would consist of positive and negative examples of the form:\n",
        "\n",
        "Input | label\n",
        "--| --\n",
        "(target_word, word_in_its_context)       |  1 \n",
        "(target_word, word_not_in_its_context)   |  0\n",
        "\n",
        "created from the sentences in a corpus. The exact number of positive and negative examples will depend on the window size, and the balance ratio of positive:negative examples.  \n",
        "\n",
        "As an example, consider the sentence: “ **The quick brown\n",
        "fox jumped over the lazy dog** ”. For the target word ' **fox** ' and a window size of 2, all the positive and negative examples drawn from this sentence would be:\n",
        "\n",
        "Input | label\n",
        "--| --\n",
        "(fox, the) | 0\n",
        "(fox, quick) | 1\n",
        "(fox, brown) | 1\n",
        "(fox, jumped) | 1\n",
        "(fox, over) |1\n",
        "(fox, lazy) | 0\n",
        "(fox, dog) | 0\n",
        "\n",
        "The model is trained to learn to predict 1 when a word is in the context of the target word (i.e.in the window of the target word) and 0 otherwise. The model thus learns the statistics of the given corpus: the frequency with two words appear together would determine how similar they are (similarity is usually measured using cosine distance). After training, the trained hidden layer weights are the word embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep2X91YdJpkG"
      },
      "source": [
        "### **0. Prepare the environment**\n",
        "\n",
        "Open Google Colab or activate the virtual environment you’ve created"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6Pi5uvTLRCo"
      },
      "source": [
        "### **1. Downloading the Corpus**\n",
        "\n",
        "Our training data will be comprised of 3 documents from the Gutenberg corpus. We can find this and other corpora in nltk https://www.nltk.org/book/ch02.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn7sWMG-KoBV",
        "outputId": "2a97e309-0177-4031-827f-0a96b62ec426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "austen = gutenberg.sents('austen-sense.txt') + gutenberg.sents('austen-emma.txt') + gutenberg.sents('austen-persuasion.txt') #what exactly is happening here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYD16ST1MIOf"
      },
      "source": [
        "**Sanity check:**\n",
        "\n",
        "This training corpus contains 16498 sentences. The following print statement should return 16498."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCH0PmPcL83f",
        "outputId": "ca60c930-a478-4e43-e705-6283b6c6e4a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length before the preprocessed output: 16498\n"
          ]
        }
      ],
      "source": [
        "print(\"length before the preprocessed output:\",len(austen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APFWiEejv9i5",
        "outputId": "41e064f8-00f9-44a7-9add-59b59767705a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[', 'Sense', 'and', 'Sensibility', 'by', 'Jane', 'Austen', '1811', ']']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "austen[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c14vElNvMGX"
      },
      "source": [
        "### **2. Preprocessing the Training Corpus**\n",
        "\n",
        "In this section, you will write code to remove special characters, empty strings, digits and\n",
        "stopwords from the sentences and put all the words into lower cases. You might also consider removing sentences with fewer than 3 words or at least empty sentences.\n",
        "\n",
        "\n",
        "**Hints:**\n",
        "*   The corpus can be accessed as you would a tokenized list, a list of lists, each inner list contains all the tokens in the sentence. Eg:\n",
        "\\>> austen[0] =['[', 'Sense', 'and', 'Sensibility', 'by','Jane', 'Austen', '1811', ']']\n",
        "\n",
        "*   the python <string> library contains a variable “punctuation”, a string containing all\n",
        "the special characters.\n",
        "\n",
        "*   You might want to write a function that takes the corpus as an argument and returns the preprocessed corpus as a list of lists.\n",
        "\n",
        " Alternatively, you can use the keras preprocessing library to preprocess the text. More information on the library can be found here:\n",
        "https://keras.io/preprocessing/text/\n",
        "\n",
        "\n",
        "**Sanity check:**\n",
        "\n",
        "After preprocessing the corpus, as a sanity check, print the following line of code. If you chose removed sentences of length with fewer than 3 words, it should be about 13651. \n",
        "\n",
        "As a test for your preprocessing function, preprocess the sample below and print the output of your function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0EvVLjlAqRF",
        "outputId": "20379d39-ba23-4959-e96e-39dbbbfca6c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install iteration_utilities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWxgQSelq37L",
        "outputId": "078c26c1-6302-4f70-ee35-b032959fe567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting iteration_utilities\n",
            "  Downloading iteration_utilities-0.11.0-cp37-cp37m-manylinux2014_x86_64.whl (283 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 61 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 71 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 81 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 92 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 102 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 112 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 122 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 143 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 153 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 163 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 174 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 184 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 194 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 204 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 215 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 225 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 235 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 245 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 256 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 266 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 276 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 283 kB 10.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: iteration-utilities\n",
            "Successfully installed iteration-utilities-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "import string\n",
        "from iteration_utilities import deepflatten"
      ],
      "metadata": {
        "id": "UeYX2C78AxJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2SIxzvwybiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78417774-87d5-4fef-e79e-9839f1e5a751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The new length of the preprocessed output: 12716\n"
          ]
        }
      ],
      "source": [
        "def preprocess_corpus(corpus):\n",
        "    corpus = [[string.lower() for string in sublist] for sublist in corpus]\n",
        "    stop_words = stopwords.words('english') #usage of stopwords\n",
        "    punctuation=list(string.punctuation) #usage of punctuation to delete unwanted punctuation\n",
        "    stop=stop_words+punctuation #now all words excepted stopwords and unwanted punctuation will be ignored\n",
        "    stop.extend(['.\"',\"--\"]) #adding more punctuations\n",
        "    filtered_sentence= [[w for w in sublist if not w in stop] for sublist in corpus]\n",
        "    filtered_sentence=[[i for i in sublist if not i.isdigit()] for sublist in filtered_sentence]\n",
        "\n",
        "\n",
        "    filtered_sentence= [sublist for sublist in filtered_sentence if len(sublist) >= 4]\n",
        "    return filtered_sentence\n",
        "\n",
        "\n",
        "normalized_corpus = preprocess_corpus(austen)\n",
        "print('The new length of the preprocessed output:',len(normalized_corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ1qxoq-8Kob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68f301e1-28cf-4885-a225-6821cdef26ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 [['[', 'Sense', 'and', 'Sensibility', 'by', 'Jane', 'Austen', '1811', ']'], ['CHAPTER', '1'], ['But', ',', 'then', ',', 'if', 'Mrs', '.', 'Dashwood', 'should', 'live', 'fifteen', 'years', 'we', 'shall', 'be', 'completely', 'taken', 'in', '.\"'], ['\"', 'Fifteen', 'years', '!']]\n",
            "\n",
            "2 [['sense', 'sensibility', 'jane', 'austen'], ['mrs', 'dashwood', 'live', 'fifteen', 'years', 'shall', 'completely', 'taken']]\n"
          ]
        }
      ],
      "source": [
        "sample = austen[:2] + austen[100:102]\n",
        "preprocessed_sample = preprocess_corpus(sample)\n",
        "\n",
        "\n",
        "print(len(sample), sample)\n",
        "print()\n",
        "print(len(preprocessed_sample), preprocessed_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rghchHA66d-V"
      },
      "source": [
        "### **3. Creating the Corpus Vocabulary and Preparing the Data**\n",
        "\n",
        "\n",
        "To prepare the data for machine learning, you will write code to prepare 3 variables:\n",
        "\n",
        "1. \\<word2idx>: a lookup table, a dictionary of (word index: word) pairs where **word index** is a unique integer assigned to every **unique word** in the corpus.\n",
        "2. \\<idx2word>: a dictionary of (token: token index), a reversal of \\<word2idx>. \n",
        "3. \\<sents_as_ids>: The input to the model cannot be text, rather, each word needs to be represented by a unique integer and each sentence an array of integers.\n",
        "\n",
        "\n",
        "The incomplete code provided below is just a guide. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_corpus_single=[] #disolving the list of list into just one list\n",
        "for list1 in preprocess_corpus(normalized_corpus):\n",
        "  for i in list1:\n",
        "    normalized_corpus_single.append(i)"
      ],
      "metadata": {
        "id": "_P4KnlWwJd50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_corpus[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH9EQtB6XgIQ",
        "outputId": "64e5dc82-5522-4dfe-a92a-ce72df584b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sense', 'sensibility', 'jane', 'austen']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, OrderedDict\n",
        "\n",
        "def prepareData(normalized_corpus):\n",
        "  normalized_corpus_dict={}\n",
        "  count=Counter(normalized_corpus)\n",
        "  count=dict(count)\n",
        "  k=0\n",
        "  for i,j in count.items():\n",
        "    normalized_corpus_dict[i]=k\n",
        "    k=k+1\n",
        "  return normalized_corpus_dict\n",
        "\n",
        "word2idx=prepareData(normalized_corpus_single)\n",
        "idx2word = dict(zip(word2idx.values(), word2idx.keys()))\n",
        "\n",
        "def prepareSentsAsId(normalized_corpus):\n",
        "  for list_inside in normalized_corpus:\n",
        "    for i in list_inside:\n",
        "      index = list_inside.index(i)\n",
        "      list_inside[index]=word2idx.get(i) \n",
        "  return normalized_corpus"
      ],
      "metadata": {
        "id": "YKviiChwlNri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### sents_as_ids "
      ],
      "metadata": {
        "id": "MPcDcCIAV6OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx"
      ],
      "metadata": {
        "id": "4rAOpsIOXwsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "569bd120-9e88-4554-d26c-717163f7f9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sense': 0,\n",
              " 'sensibility': 1,\n",
              " 'jane': 2,\n",
              " 'austen': 3,\n",
              " 'family': 4,\n",
              " 'dashwood': 5,\n",
              " 'long': 6,\n",
              " 'settled': 7,\n",
              " 'sussex': 8,\n",
              " 'estate': 9,\n",
              " 'large': 10,\n",
              " 'residence': 11,\n",
              " 'norland': 12,\n",
              " 'park': 13,\n",
              " 'centre': 14,\n",
              " 'property': 15,\n",
              " 'many': 16,\n",
              " 'generations': 17,\n",
              " 'lived': 18,\n",
              " 'respectable': 19,\n",
              " 'manner': 20,\n",
              " 'engage': 21,\n",
              " 'general': 22,\n",
              " 'good': 23,\n",
              " 'opinion': 24,\n",
              " 'surrounding': 25,\n",
              " 'acquaintance': 26,\n",
              " 'late': 27,\n",
              " 'owner': 28,\n",
              " 'single': 29,\n",
              " 'man': 30,\n",
              " 'advanced': 31,\n",
              " 'age': 32,\n",
              " 'years': 33,\n",
              " 'life': 34,\n",
              " 'constant': 35,\n",
              " 'companion': 36,\n",
              " 'housekeeper': 37,\n",
              " 'sister': 38,\n",
              " 'death': 39,\n",
              " 'happened': 40,\n",
              " 'ten': 41,\n",
              " 'produced': 42,\n",
              " 'great': 43,\n",
              " 'alteration': 44,\n",
              " 'home': 45,\n",
              " 'supply': 46,\n",
              " 'loss': 47,\n",
              " 'invited': 48,\n",
              " 'received': 49,\n",
              " 'house': 50,\n",
              " 'nephew': 51,\n",
              " 'mr': 52,\n",
              " 'henry': 53,\n",
              " 'legal': 54,\n",
              " 'inheritor': 55,\n",
              " 'person': 56,\n",
              " 'intended': 57,\n",
              " 'bequeath': 58,\n",
              " 'society': 59,\n",
              " 'niece': 60,\n",
              " 'children': 61,\n",
              " 'old': 62,\n",
              " 'gentleman': 63,\n",
              " 'days': 64,\n",
              " 'comfortably': 65,\n",
              " 'spent': 66,\n",
              " 'attention': 67,\n",
              " 'mrs': 68,\n",
              " 'wishes': 69,\n",
              " 'proceeded': 70,\n",
              " 'merely': 71,\n",
              " 'interest': 72,\n",
              " 'goodness': 73,\n",
              " 'heart': 74,\n",
              " 'gave': 75,\n",
              " 'every': 76,\n",
              " 'degree': 77,\n",
              " 'solid': 78,\n",
              " 'comfort': 79,\n",
              " 'could': 80,\n",
              " 'receive': 81,\n",
              " 'cheerfulness': 82,\n",
              " 'added': 83,\n",
              " 'relish': 84,\n",
              " 'existence': 85,\n",
              " 'former': 86,\n",
              " 'marriage': 87,\n",
              " 'one': 88,\n",
              " 'son': 89,\n",
              " 'present': 90,\n",
              " 'lady': 91,\n",
              " 'three': 92,\n",
              " 'daughters': 93,\n",
              " 'steady': 94,\n",
              " 'young': 95,\n",
              " 'amply': 96,\n",
              " 'provided': 97,\n",
              " 'fortune': 98,\n",
              " 'mother': 99,\n",
              " 'half': 100,\n",
              " 'devolved': 101,\n",
              " 'coming': 102,\n",
              " 'likewise': 103,\n",
              " 'soon': 104,\n",
              " 'afterwards': 105,\n",
              " 'wealth': 106,\n",
              " 'therefore': 107,\n",
              " 'succession': 108,\n",
              " 'really': 109,\n",
              " 'important': 110,\n",
              " 'sisters': 111,\n",
              " 'independent': 112,\n",
              " 'might': 113,\n",
              " 'arise': 114,\n",
              " 'father': 115,\n",
              " 'inheriting': 116,\n",
              " 'small': 117,\n",
              " 'nothing': 118,\n",
              " 'seven': 119,\n",
              " 'thousand': 120,\n",
              " 'pounds': 121,\n",
              " 'disposal': 122,\n",
              " 'remaining': 123,\n",
              " 'moiety': 124,\n",
              " 'first': 125,\n",
              " 'wife': 126,\n",
              " 'also': 127,\n",
              " 'secured': 128,\n",
              " 'child': 129,\n",
              " 'died': 130,\n",
              " 'read': 131,\n",
              " 'like': 132,\n",
              " 'almost': 133,\n",
              " 'much': 134,\n",
              " 'disappointment': 135,\n",
              " 'pleasure': 136,\n",
              " 'neither': 137,\n",
              " 'unjust': 138,\n",
              " 'ungrateful': 139,\n",
              " 'leave': 140,\n",
              " ';--': 141,\n",
              " 'left': 142,\n",
              " 'terms': 143,\n",
              " 'destroyed': 144,\n",
              " 'value': 145,\n",
              " 'bequest': 146,\n",
              " 'wished': 147,\n",
              " 'sake': 148,\n",
              " 'four': 149,\n",
              " 'way': 150,\n",
              " 'power': 151,\n",
              " 'providing': 152,\n",
              " 'dear': 153,\n",
              " 'needed': 154,\n",
              " 'provision': 155,\n",
              " 'charge': 156,\n",
              " 'sale': 157,\n",
              " 'valuable': 158,\n",
              " 'woods': 159,\n",
              " 'whole': 160,\n",
              " 'tied': 161,\n",
              " 'benefit': 162,\n",
              " 'occasional': 163,\n",
              " 'visits': 164,\n",
              " 'far': 165,\n",
              " 'gained': 166,\n",
              " 'affections': 167,\n",
              " 'uncle': 168,\n",
              " 'attractions': 169,\n",
              " 'means': 170,\n",
              " 'unusual': 171,\n",
              " 'two': 172,\n",
              " 'imperfect': 173,\n",
              " 'articulation': 174,\n",
              " 'earnest': 175,\n",
              " 'desire': 176,\n",
              " 'cunning': 177,\n",
              " 'tricks': 178,\n",
              " 'deal': 179,\n",
              " 'noise': 180,\n",
              " 'outweigh': 181,\n",
              " 'meant': 182,\n",
              " 'unkind': 183,\n",
              " 'however': 184,\n",
              " 'mark': 185,\n",
              " 'affection': 186,\n",
              " 'girls': 187,\n",
              " 'piece': 188,\n",
              " 'severe': 189,\n",
              " 'temper': 190,\n",
              " 'cheerful': 191,\n",
              " 'sanguine': 192,\n",
              " 'reasonably': 193,\n",
              " 'hope': 194,\n",
              " 'live': 195,\n",
              " 'living': 196,\n",
              " 'economically': 197,\n",
              " 'lay': 198,\n",
              " 'considerable': 199,\n",
              " 'sum': 200,\n",
              " 'produce': 201,\n",
              " 'already': 202,\n",
              " 'capable': 203,\n",
              " 'immediate': 204,\n",
              " 'improvement': 205,\n",
              " 'tardy': 206,\n",
              " 'twelvemonth': 207,\n",
              " 'survived': 208,\n",
              " 'longer': 209,\n",
              " 'including': 210,\n",
              " 'legacies': 211,\n",
              " 'remained': 212,\n",
              " 'widow': 213,\n",
              " 'sent': 214,\n",
              " 'danger': 215,\n",
              " 'known': 216,\n",
              " 'recommended': 217,\n",
              " 'strength': 218,\n",
              " 'urgency': 219,\n",
              " 'illness': 220,\n",
              " 'command': 221,\n",
              " 'law': 222,\n",
              " 'john': 223,\n",
              " 'strong': 224,\n",
              " 'feelings': 225,\n",
              " 'rest': 226,\n",
              " 'affected': 227,\n",
              " 'recommendation': 228,\n",
              " 'nature': 229,\n",
              " 'time': 230,\n",
              " 'promised': 231,\n",
              " 'thing': 232,\n",
              " 'make': 233,\n",
              " 'comfortable': 234,\n",
              " 'rendered': 235,\n",
              " 'easy': 236,\n",
              " 'assurance': 237,\n",
              " 'leisure': 238,\n",
              " 'consider': 239,\n",
              " 'prudently': 240,\n",
              " 'ill': 241,\n",
              " 'disposed': 242,\n",
              " 'unless': 243,\n",
              " 'rather': 244,\n",
              " 'cold': 245,\n",
              " 'hearted': 246,\n",
              " 'selfish': 247,\n",
              " 'well': 248,\n",
              " 'respected': 249,\n",
              " 'conducted': 250,\n",
              " 'propriety': 251,\n",
              " 'discharge': 252,\n",
              " 'ordinary': 253,\n",
              " 'duties': 254,\n",
              " 'married': 255,\n",
              " 'amiable': 256,\n",
              " 'woman': 257,\n",
              " 'made': 258,\n",
              " 'still': 259,\n",
              " ':--': 260,\n",
              " 'even': 261,\n",
              " 'fond': 262,\n",
              " 'caricature': 263,\n",
              " 'narrow': 264,\n",
              " 'minded': 265,\n",
              " 'promise': 266,\n",
              " 'meditated': 267,\n",
              " 'within': 268,\n",
              " 'increase': 269,\n",
              " 'fortunes': 270,\n",
              " 'prospect': 271,\n",
              " 'year': 272,\n",
              " 'addition': 273,\n",
              " 'income': 274,\n",
              " 'besides': 275,\n",
              " 'warmed': 276,\n",
              " 'feel': 277,\n",
              " 'generosity': 278,\n",
              " '.--': 279,\n",
              " 'yes': 280,\n",
              " 'would': 281,\n",
              " 'give': 282,\n",
              " 'liberal': 283,\n",
              " 'handsome': 284,\n",
              " 'enough': 285,\n",
              " 'completely': 286,\n",
              " 'spare': 287,\n",
              " 'little': 288,\n",
              " 'inconvenience': 289,\n",
              " 'thought': 290,\n",
              " 'day': 291,\n",
              " 'successively': 292,\n",
              " 'repent': 293,\n",
              " 'sooner': 294,\n",
              " 'funeral': 295,\n",
              " 'without': 296,\n",
              " 'sending': 297,\n",
              " 'notice': 298,\n",
              " 'intention': 299,\n",
              " 'arrived': 300,\n",
              " 'attendants': 301,\n",
              " 'dispute': 302,\n",
              " 'right': 303,\n",
              " 'come': 304,\n",
              " 'husband': 305,\n",
              " 'moment': 306,\n",
              " 'decease': 307,\n",
              " 'indelicacy': 308,\n",
              " 'conduct': 309,\n",
              " 'greater': 310,\n",
              " 'situation': 311,\n",
              " 'common': 312,\n",
              " 'must': 313,\n",
              " 'highly': 314,\n",
              " 'unpleasing': 315,\n",
              " 'mind': 316,\n",
              " 'honor': 317,\n",
              " 'keen': 318,\n",
              " 'romantic': 319,\n",
              " 'offence': 320,\n",
              " 'kind': 321,\n",
              " 'whomsoever': 322,\n",
              " 'given': 323,\n",
              " 'source': 324,\n",
              " 'immoveable': 325,\n",
              " 'disgust': 326,\n",
              " 'never': 327,\n",
              " 'favourite': 328,\n",
              " 'opportunity': 329,\n",
              " 'till': 330,\n",
              " 'shewing': 331,\n",
              " 'people': 332,\n",
              " 'act': 333,\n",
              " 'occasion': 334,\n",
              " 'required': 335,\n",
              " 'acutely': 336,\n",
              " 'ungracious': 337,\n",
              " 'behaviour': 338,\n",
              " 'earnestly': 339,\n",
              " 'despise': 340,\n",
              " 'daughter': 341,\n",
              " 'arrival': 342,\n",
              " 'latter': 343,\n",
              " 'quitted': 344,\n",
              " 'ever': 345,\n",
              " 'entreaty': 346,\n",
              " 'eldest': 347,\n",
              " 'girl': 348,\n",
              " 'induced': 349,\n",
              " 'reflect': 350,\n",
              " 'going': 351,\n",
              " 'tender': 352,\n",
              " 'love': 353,\n",
              " 'determined': 354,\n",
              " 'stay': 355,\n",
              " 'sakes': 356,\n",
              " 'avoid': 357,\n",
              " 'breach': 358,\n",
              " 'brother': 359,\n",
              " 'elinor': 360,\n",
              " 'whose': 361,\n",
              " 'advice': 362,\n",
              " 'effectual': 363,\n",
              " 'possessed': 364,\n",
              " 'understanding': 365,\n",
              " 'coolness': 366,\n",
              " 'judgment': 367,\n",
              " 'qualified': 368,\n",
              " 'though': 369,\n",
              " 'nineteen': 370,\n",
              " 'counsellor': 371,\n",
              " 'enabled': 372,\n",
              " 'frequently': 373,\n",
              " 'counteract': 374,\n",
              " 'advantage': 375,\n",
              " 'eagerness': 376,\n",
              " 'generally': 377,\n",
              " 'led': 378,\n",
              " 'imprudence': 379,\n",
              " 'excellent': 380,\n",
              " 'disposition': 381,\n",
              " 'affectionate': 382,\n",
              " 'knew': 383,\n",
              " 'govern': 384,\n",
              " 'knowledge': 385,\n",
              " 'yet': 386,\n",
              " 'learn': 387,\n",
              " 'resolved': 388,\n",
              " 'taught': 389,\n",
              " 'marianne': 390,\n",
              " 'abilities': 391,\n",
              " 'respects': 392,\n",
              " 'quite': 393,\n",
              " 'equal': 394,\n",
              " 'sensible': 395,\n",
              " 'clever': 396,\n",
              " 'eager': 397,\n",
              " 'everything': 398,\n",
              " 'sorrows': 399,\n",
              " 'joys': 400,\n",
              " 'moderation': 401,\n",
              " 'generous': 402,\n",
              " 'interesting': 403,\n",
              " 'prudent': 404,\n",
              " 'resemblance': 405,\n",
              " 'strikingly': 406,\n",
              " 'saw': 407,\n",
              " 'concern': 408,\n",
              " 'excess': 409,\n",
              " 'valued': 410,\n",
              " 'cherished': 411,\n",
              " 'agony': 412,\n",
              " 'grief': 413,\n",
              " 'overpowered': 414,\n",
              " 'voluntarily': 415,\n",
              " 'renewed': 416,\n",
              " 'sought': 417,\n",
              " 'created': 418,\n",
              " 'wholly': 419,\n",
              " 'sorrow': 420,\n",
              " 'seeking': 421,\n",
              " 'wretchedness': 422,\n",
              " 'reflection': 423,\n",
              " 'afford': 424,\n",
              " 'admitting': 425,\n",
              " 'consolation': 426,\n",
              " 'future': 427,\n",
              " 'deeply': 428,\n",
              " 'afflicted': 429,\n",
              " 'struggle': 430,\n",
              " 'exert': 431,\n",
              " 'consult': 432,\n",
              " 'treat': 433,\n",
              " 'proper': 434,\n",
              " 'strive': 435,\n",
              " 'rouse': 436,\n",
              " 'similar': 437,\n",
              " 'exertion': 438,\n",
              " 'encourage': 439,\n",
              " 'forbearance': 440,\n",
              " 'margaret': 441,\n",
              " 'humored': 442,\n",
              " 'imbibed': 443,\n",
              " 'romance': 444,\n",
              " 'thirteen': 445,\n",
              " 'bid': 446,\n",
              " 'fair': 447,\n",
              " 'period': 448,\n",
              " 'installed': 449,\n",
              " 'mistress': 450,\n",
              " 'degraded': 451,\n",
              " 'condition': 452,\n",
              " 'visitors': 453,\n",
              " 'treated': 454,\n",
              " 'quiet': 455,\n",
              " 'civility': 456,\n",
              " 'kindness': 457,\n",
              " 'towards': 458,\n",
              " 'anybody': 459,\n",
              " 'beyond': 460,\n",
              " 'pressed': 461,\n",
              " 'earnestness': 462,\n",
              " 'plan': 463,\n",
              " 'appeared': 464,\n",
              " 'eligible': 465,\n",
              " 'accommodate': 466,\n",
              " 'neighbourhood': 467,\n",
              " 'invitation': 468,\n",
              " 'accepted': 469,\n",
              " 'continuance': 470,\n",
              " 'place': 471,\n",
              " 'reminded': 472,\n",
              " 'delight': 473,\n",
              " 'exactly': 474,\n",
              " 'suited': 475,\n",
              " 'seasons': 476,\n",
              " 'possess': 477,\n",
              " 'expectation': 478,\n",
              " 'happiness': 479,\n",
              " 'equally': 480,\n",
              " 'carried': 481,\n",
              " 'away': 482,\n",
              " 'fancy': 483,\n",
              " 'alloy': 484,\n",
              " 'approve': 485,\n",
              " 'take': 486,\n",
              " 'boy': 487,\n",
              " 'impoverishing': 488,\n",
              " 'dreadful': 489,\n",
              " 'answer': 490,\n",
              " 'rob': 491,\n",
              " 'possible': 492,\n",
              " 'claim': 493,\n",
              " 'miss': 494,\n",
              " 'dashwoods': 495,\n",
              " 'related': 496,\n",
              " 'blood': 497,\n",
              " 'considered': 498,\n",
              " 'relationship': 499,\n",
              " 'amount': 500,\n",
              " 'supposed': 501,\n",
              " 'exist': 502,\n",
              " 'different': 503,\n",
              " 'marriages': 504,\n",
              " 'ruin': 505,\n",
              " 'poor': 506,\n",
              " 'harry': 507,\n",
              " 'giving': 508,\n",
              " 'money': 509,\n",
              " 'last': 510,\n",
              " 'request': 511,\n",
              " ',\"': 512,\n",
              " 'replied': 513,\n",
              " 'assist': 514,\n",
              " 'know': 515,\n",
              " 'talking': 516,\n",
              " 'dare': 517,\n",
              " 'say': 518,\n",
              " 'light': 519,\n",
              " 'headed': 520,\n",
              " 'senses': 521,\n",
              " 'begging': 522,\n",
              " 'stipulate': 523,\n",
              " 'particular': 524,\n",
              " 'fanny': 525,\n",
              " 'requested': 526,\n",
              " 'perhaps': 527,\n",
              " 'hardly': 528,\n",
              " 'suppose': 529,\n",
              " 'neglect': 530,\n",
              " 'less': 531,\n",
              " 'least': 532,\n",
              " 'performed': 533,\n",
              " 'something': 534,\n",
              " 'done': 535,\n",
              " 'whenever': 536,\n",
              " 'settle': 537,\n",
              " 'new': 538,\n",
              " 'let': 539,\n",
              " 'need': 540,\n",
              " 'parted': 541,\n",
              " 'return': 542,\n",
              " 'marry': 543,\n",
              " 'gone': 544,\n",
              " 'indeed': 545,\n",
              " 'restored': 546,\n",
              " '--\"': 547,\n",
              " 'sure': 548,\n",
              " 'said': 549,\n",
              " 'gravely': 550,\n",
              " 'difference': 551,\n",
              " 'may': 552,\n",
              " 'regret': 553,\n",
              " 'numerous': 554,\n",
              " 'instance': 555,\n",
              " 'convenient': 556,\n",
              " 'better': 557,\n",
              " 'parties': 558,\n",
              " 'diminished': 559,\n",
              " 'five': 560,\n",
              " 'hundred': 561,\n",
              " 'prodigious': 562,\n",
              " '!\"': 563,\n",
              " 'earth': 564,\n",
              " '!--': 565,\n",
              " 'spirit': 566,\n",
              " 'wish': 567,\n",
              " 'mean': 568,\n",
              " 'occasions': 569,\n",
              " 'think': 570,\n",
              " 'expect': 571,\n",
              " 'knowing': 572,\n",
              " 'expectations': 573,\n",
              " 'question': 574,\n",
              " 'certainly': 575,\n",
              " 'mine': 576,\n",
              " 'strikes': 577,\n",
              " 'want': 578,\n",
              " 'divided': 579,\n",
              " 'amongst': 580,\n",
              " 'together': 581,\n",
              " 'true': 582,\n",
              " 'whether': 583,\n",
              " 'upon': 584,\n",
              " 'advisable': 585,\n",
              " 'lives': 586,\n",
              " 'annuity': 587,\n",
              " 'effects': 588,\n",
              " 'perfectly': 589,\n",
              " 'hesitated': 590,\n",
              " 'consent': 591,\n",
              " 'parting': 592,\n",
              " 'fifteen': 593,\n",
              " 'shall': 594,\n",
              " 'taken': 595,\n",
              " 'cannot': 596,\n",
              " 'worth': 597,\n",
              " 'purchase': 598,\n",
              " 'observe': 599,\n",
              " 'always': 600,\n",
              " 'paid': 601,\n",
              " 'stout': 602,\n",
              " 'healthy': 603,\n",
              " 'forty': 604,\n",
              " 'serious': 605,\n",
              " 'business': 606,\n",
              " 'comes': 607,\n",
              " 'getting': 608,\n",
              " 'rid': 609,\n",
              " 'trouble': 610,\n",
              " 'annuities': 611,\n",
              " 'clogged': 612,\n",
              " 'payment': 613,\n",
              " 'superannuated': 614,\n",
              " 'servants': 615,\n",
              " 'amazing': 616,\n",
              " 'disagreeable': 617,\n",
              " 'found': 618,\n",
              " 'twice': 619,\n",
              " 'turned': 620,\n",
              " 'perpetual': 621,\n",
              " 'claims': 622,\n",
              " 'otherwise': 623,\n",
              " 'entirely': 624,\n",
              " 'restriction': 625,\n",
              " 'whatever': 626,\n",
              " 'abhorrence': 627,\n",
              " 'pin': 628,\n",
              " 'world': 629,\n",
              " 'unpleasant': 630,\n",
              " 'yearly': 631,\n",
              " 'drains': 632,\n",
              " 'justly': 633,\n",
              " 'says': 634,\n",
              " 'regular': 635,\n",
              " 'rent': 636,\n",
              " 'desirable': 637,\n",
              " 'takes': 638,\n",
              " 'independence': 639,\n",
              " 'secure': 640,\n",
              " 'expected': 641,\n",
              " 'raises': 642,\n",
              " 'gratitude': 643,\n",
              " 'discretion': 644,\n",
              " 'bind': 645,\n",
              " 'allow': 646,\n",
              " 'inconvenient': 647,\n",
              " 'fifty': 648,\n",
              " 'expenses': 649,\n",
              " 'believe': 650,\n",
              " 'case': 651,\n",
              " 'occasionally': 652,\n",
              " 'assistance': 653,\n",
              " 'allowance': 654,\n",
              " 'enlarge': 655,\n",
              " 'style': 656,\n",
              " 'felt': 657,\n",
              " 'larger': 658,\n",
              " 'sixpence': 659,\n",
              " 'richer': 660,\n",
              " 'end': 661,\n",
              " 'best': 662,\n",
              " 'prevent': 663,\n",
              " 'distressed': 664,\n",
              " 'discharging': 665,\n",
              " 'truth': 666,\n",
              " 'convinced': 667,\n",
              " 'idea': 668,\n",
              " 'looking': 669,\n",
              " 'helping': 670,\n",
              " 'move': 671,\n",
              " 'things': 672,\n",
              " 'presents': 673,\n",
              " 'fish': 674,\n",
              " 'game': 675,\n",
              " 'forth': 676,\n",
              " 'season': 677,\n",
              " 'farther': 678,\n",
              " 'strange': 679,\n",
              " 'unreasonable': 680,\n",
              " 'excessively': 681,\n",
              " 'belonging': 682,\n",
              " 'brings': 683,\n",
              " 'course': 684,\n",
              " 'pay': 685,\n",
              " 'board': 686,\n",
              " 'altogether': 687,\n",
              " 'women': 688,\n",
              " '?--': 689,\n",
              " 'cheap': 690,\n",
              " 'carriage': 691,\n",
              " 'horses': 692,\n",
              " 'keep': 693,\n",
              " 'company': 694,\n",
              " 'imagine': 695,\n",
              " 'spend': 696,\n",
              " 'absurd': 697,\n",
              " 'able': 698,\n",
              " 'word': 699,\n",
              " 'clearly': 700,\n",
              " 'understand': 701,\n",
              " 'strictly': 702,\n",
              " 'fulfil': 703,\n",
              " 'engagement': 704,\n",
              " 'acts': 705,\n",
              " 'described': 706,\n",
              " 'removes': 707,\n",
              " 'another': 708,\n",
              " 'services': 709,\n",
              " 'readily': 710,\n",
              " 'furniture': 711,\n",
              " 'acceptable': 712,\n",
              " 'returned': 713,\n",
              " 'moved': 714,\n",
              " 'stanhill': 715,\n",
              " 'sold': 716,\n",
              " 'china': 717,\n",
              " 'plate': 718,\n",
              " 'linen': 719,\n",
              " 'saved': 720,\n",
              " 'fitted': 721,\n",
              " 'pleasant': 722,\n",
              " 'stock': 723,\n",
              " 'set': 724,\n",
              " 'breakfast': 725,\n",
              " 'belongs': 726,\n",
              " 'owe': 727,\n",
              " 'intentions': 728,\n",
              " 'decision': 729,\n",
              " 'wanting': 730,\n",
              " 'finally': 731,\n",
              " 'absolutely': 732,\n",
              " 'unnecessary': 733,\n",
              " 'indecorous': 734,\n",
              " 'neighbourly': 735,\n",
              " 'pointed': 736,\n",
              " 'several': 737,\n",
              " 'months': 738,\n",
              " 'disinclination': 739,\n",
              " 'sight': 740,\n",
              " 'spot': 741,\n",
              " 'ceased': 742,\n",
              " 'raise': 743,\n",
              " 'violent': 744,\n",
              " 'emotion': 745,\n",
              " 'spirits': 746,\n",
              " 'began': 747,\n",
              " 'revive': 748,\n",
              " 'became': 749,\n",
              " 'heightening': 750,\n",
              " 'affliction': 751,\n",
              " 'melancholy': 752,\n",
              " 'remembrances': 753,\n",
              " 'impatient': 754,\n",
              " 'indefatigable': 755,\n",
              " 'inquiries': 756,\n",
              " 'suitable': 757,\n",
              " 'dwelling': 758,\n",
              " 'remove': 759,\n",
              " 'beloved': 760,\n",
              " 'impossible': 761,\n",
              " 'hear': 762,\n",
              " 'answered': 763,\n",
              " 'notions': 764,\n",
              " 'ease': 765,\n",
              " 'prudence': 766,\n",
              " 'steadier': 767,\n",
              " 'rejected': 768,\n",
              " 'houses': 769,\n",
              " 'approved': 770,\n",
              " 'informed': 771,\n",
              " 'solemn': 772,\n",
              " 'part': 773,\n",
              " 'favour': 774,\n",
              " 'earthly': 775,\n",
              " 'reflections': 776,\n",
              " 'doubted': 777,\n",
              " 'sincerity': 778,\n",
              " 'satisfaction': 779,\n",
              " 'persuaded': 780,\n",
              " 'smaller': 781,\n",
              " '7000l': 782,\n",
              " 'support': 783,\n",
              " 'affluence': 784,\n",
              " 'rejoiced': 785,\n",
              " 'reproached': 786,\n",
              " 'merit': 787,\n",
              " 'believing': 788,\n",
              " 'incapable': 789,\n",
              " 'attentive': 790,\n",
              " 'welfare': 791,\n",
              " 'firmly': 792,\n",
              " 'relied': 793,\n",
              " 'liberality': 794,\n",
              " 'contempt': 795,\n",
              " 'early': 796,\n",
              " 'increased': 797,\n",
              " 'character': 798,\n",
              " 'afforded': 799,\n",
              " 'spite': 800,\n",
              " 'consideration': 801,\n",
              " 'politeness': 802,\n",
              " 'maternal': 803,\n",
              " 'side': 804,\n",
              " 'ladies': 805,\n",
              " 'circumstance': 806,\n",
              " 'occurred': 807,\n",
              " 'eligibility': 808,\n",
              " 'according': 809,\n",
              " 'opinions': 810,\n",
              " 'growing': 811,\n",
              " 'attachment': 812,\n",
              " 'pleasing': 813,\n",
              " 'introduced': 814,\n",
              " 'establishment': 815,\n",
              " 'since': 816,\n",
              " 'greatest': 817,\n",
              " 'mothers': 818,\n",
              " 'encouraged': 819,\n",
              " 'intimacy': 820,\n",
              " 'motives': 821,\n",
              " 'edward': 822,\n",
              " 'ferrars': 823,\n",
              " 'rich': 824,\n",
              " 'repressed': 825,\n",
              " 'except': 826,\n",
              " 'trifling': 827,\n",
              " 'depended': 828,\n",
              " 'alike': 829,\n",
              " 'uninfluenced': 830,\n",
              " 'either': 831,\n",
              " 'loved': 832,\n",
              " 'partiality': 833,\n",
              " 'contrary': 834,\n",
              " 'doctrine': 835,\n",
              " 'couple': 836,\n",
              " 'asunder': 837,\n",
              " 'attracted': 838,\n",
              " 'acknowledged': 839,\n",
              " 'comprehension': 840,\n",
              " 'peculiar': 841,\n",
              " 'graces': 842,\n",
              " 'address': 843,\n",
              " 'manners': 844,\n",
              " 'diffident': 845,\n",
              " 'justice': 846,\n",
              " 'natural': 847,\n",
              " 'shyness': 848,\n",
              " 'overcome': 849,\n",
              " 'indication': 850,\n",
              " 'open': 851,\n",
              " 'education': 852,\n",
              " 'longed': 853,\n",
              " 'see': 854,\n",
              " 'distinguished': 855,\n",
              " 'wanted': 856,\n",
              " 'fine': 857,\n",
              " 'figure': 858,\n",
              " 'political': 859,\n",
              " 'concerns': 860,\n",
              " 'get': 861,\n",
              " 'parliament': 862,\n",
              " 'connected': 863,\n",
              " 'men': 864,\n",
              " 'superior': 865,\n",
              " 'blessings': 866,\n",
              " 'attained': 867,\n",
              " 'quieted': 868,\n",
              " 'ambition': 869,\n",
              " 'driving': 870,\n",
              " 'barouche': 871,\n",
              " 'turn': 872,\n",
              " 'barouches': 873,\n",
              " 'centered': 874,\n",
              " 'domestic': 875,\n",
              " 'private': 876,\n",
              " 'fortunately': 877,\n",
              " 'younger': 878,\n",
              " 'promising': 879,\n",
              " 'staying': 880,\n",
              " 'weeks': 881,\n",
              " 'engaged': 882,\n",
              " 'careless': 883,\n",
              " 'objects': 884,\n",
              " 'unobtrusive': 885,\n",
              " 'liked': 886,\n",
              " 'disturb': 887,\n",
              " 'timed': 888,\n",
              " 'conversation': 889,\n",
              " 'called': 890,\n",
              " 'chanced': 891,\n",
              " 'contrast': 892,\n",
              " 'forcibly': 893,\n",
              " 'unlike': 894,\n",
              " 'sentiment': 895,\n",
              " 'approbation': 896,\n",
              " 'inferior': 897,\n",
              " 'separate': 898,\n",
              " 'esteem': 899,\n",
              " 'took': 900,\n",
              " 'pains': 901,\n",
              " 'acquainted': 902,\n",
              " 'attaching': 903,\n",
              " 'banished': 904,\n",
              " 'reserve': 905,\n",
              " 'speedily': 906,\n",
              " 'comprehended': 907,\n",
              " 'merits': 908,\n",
              " 'persuasion': 909,\n",
              " 'regard': 910,\n",
              " 'assisted': 911,\n",
              " 'penetration': 912,\n",
              " 'assured': 913,\n",
              " 'quietness': 914,\n",
              " 'militated': 915,\n",
              " 'established': 916,\n",
              " 'ideas': 917,\n",
              " 'ought': 918,\n",
              " 'uninteresting': 919,\n",
              " 'warm': 920,\n",
              " 'perceive': 921,\n",
              " 'symptom': 922,\n",
              " 'certain': 923,\n",
              " 'looked': 924,\n",
              " 'forward': 925,\n",
              " 'rapidly': 926,\n",
              " 'approaching': 927,\n",
              " 'probability': 928,\n",
              " 'mamma': 929,\n",
              " '?\"': 930,\n",
              " 'miles': 931,\n",
              " 'meet': 932,\n",
              " 'gain': 933,\n",
              " 'real': 934,\n",
              " 'highest': 935,\n",
              " 'look': 936,\n",
              " 'grave': 937,\n",
              " 'disapprove': 938,\n",
              " 'choice': 939,\n",
              " 'surprise': 940,\n",
              " 'tenderly': 941,\n",
              " 'striking': 942,\n",
              " 'none': 943,\n",
              " 'grace': 944,\n",
              " 'seriously': 945,\n",
              " 'attach': 946,\n",
              " 'eyes': 947,\n",
              " 'fire': 948,\n",
              " 'announce': 949,\n",
              " 'virtue': 950,\n",
              " 'intelligence': 951,\n",
              " 'afraid': 952,\n",
              " 'taste': 953,\n",
              " 'music': 954,\n",
              " 'seems': 955,\n",
              " 'scarcely': 956,\n",
              " 'attract': 957,\n",
              " 'admires': 958,\n",
              " 'drawings': 959,\n",
              " 'admiration': 960,\n",
              " 'evident': 961,\n",
              " 'frequent': 962,\n",
              " 'draws': 963,\n",
              " 'fact': 964,\n",
              " 'knows': 965,\n",
              " 'matter': 966,\n",
              " 'satisfy': 967,\n",
              " 'characters': 968,\n",
              " 'united': 969,\n",
              " 'happy': 970,\n",
              " 'point': 971,\n",
              " 'coincide': 972,\n",
              " 'enter': 973,\n",
              " 'books': 974,\n",
              " 'charm': 975,\n",
              " 'us': 976,\n",
              " 'mama': 977,\n",
              " 'spiritless': 978,\n",
              " 'tame': 979,\n",
              " 'reading': 980,\n",
              " 'night': 981,\n",
              " 'bore': 982,\n",
              " 'composure': 983,\n",
              " 'seemed': 984,\n",
              " 'seat': 985,\n",
              " 'beautiful': 986,\n",
              " 'lines': 987,\n",
              " 'driven': 988,\n",
              " 'wild': 989,\n",
              " 'pronounced': 990,\n",
              " 'impenetrable': 991,\n",
              " 'calmness': 992,\n",
              " 'indifference': 993,\n",
              " 'simple': 994,\n",
              " 'elegant': 995,\n",
              " 'prose': 996,\n",
              " 'cowper': 997,\n",
              " 'nay': 998,\n",
              " 'animated': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ---------------------"
      ],
      "metadata": {
        "id": "VH3dfzMrV9tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of unique words:', len(word2idx))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Rqami84Va1N",
        "outputId": "be064ba6-7e33-45ae-d207-1702fdb0155f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words: 10285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nSample word2idx: ', list(word2idx.items())[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N0E7hwpVeKV",
        "outputId": "0497ba9f-193b-426f-d869-8fba17131861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample word2idx:  [('sense', 0), ('sensibility', 1), ('jane', 2), ('austen', 3), ('family', 4), ('dashwood', 5), ('long', 6), ('settled', 7), ('sussex', 8), ('estate', 9)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nSample idx2word:', list(idx2word.items())[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-J1bZN1Vi3f",
        "outputId": "60a2e5b4-30a2-45f8-9467-e456a1b399e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample idx2word: [(0, 'sense'), (1, 'sensibility'), (2, 'jane'), (3, 'austen'), (4, 'family'), (5, 'dashwood'), (6, 'long'), (7, 'settled'), (8, 'sussex'), (9, 'estate')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sents_as_ids=normalized_corpus_test_1\n",
        "print('\\nSample sents_as_id:', prepareSentsAsId(preprocessed_sample))"
      ],
      "metadata": {
        "id": "Xk88em_5VoRi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc9c0fcc-2be0-4bc0-cc03-655ea512dc51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample sents_as_id: [[0, 1, 2, 3], [68, 5, 195, 593, 33, 594, 286, 595]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCa7VGc59bCD"
      },
      "source": [
        "**Sanity Check**\n",
        "\n",
        "Copy and run the following lines of code:\n",
        "\n",
        "```\n",
        "print('Number of unique words:', len(word2idx)**\n",
        "```\n",
        "Returns a number between 9800 and 1200, the exact number depends on your preprocessing step. \n",
        "\n",
        "```\n",
        "print('\\nSample word2idx: ', list(word2idx.items())[:10])\n",
        "```\n",
        "\n",
        "Sample word2idx:  [('sense', 0), ('sensibility', 1), ('jane', 2), ('austen', 3), ('the', 4), ('family', 5), ('dashwood', 6), ('long', 7), ('settled', 8), ('sussex', 9)]\n",
        "\n",
        "\n",
        "```\n",
        "print('\\nSample idx2word:', list(idx2word.items())[:10])**\n",
        "```\n",
        "\n",
        "Sample idx2word: [(1, 'could'), (2, 'would'), (3, 'mr'), (4, 'mrs'), (5, 'must'), (6, 'said'), (7, 'one'), (8, 'much'), (9, 'miss'), (10, 'every')]\n",
        "\n",
        "```\n",
        "print('\\nSample sents_as_id:', prepareSentsAsId(preprocessed_sample))\n",
        "```\n",
        "\n",
        "Sample sents_as_id: [[0, 1, 2, 3], [41, 72, 6, 201, 619, 35, 620, 296, 621]]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqw71xzzeYSm"
      },
      "source": [
        "After you have created the three variables, set the <vocab_size> and <embed_size> variables with the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGsp5Lij8qUd"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = len(word2idx) \n",
        "EMBED_SIZE = 100 # We are creating 100D embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_yjd5kgqiZi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bda8a487-3f7f-46eb-9f42-82b2347cb5cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words: 10285\n"
          ]
        }
      ],
      "source": [
        "print('Number of unique words:', len(word2idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXhJNdf9TIed"
      },
      "source": [
        "### **4. Generating training instances**\n",
        "\n",
        "\n",
        "In this section we would generate the training examples of the format shown in introduction using the keras skip-gram generator https://keras.io/preprocessing/sequence/ "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import skipgrams\n",
        "\n",
        "# generate skip-grams\n",
        "wids = [[word2idx[w] for w in doc] for doc in normalized_corpus]\n",
        "skip_grams = [skipgrams(wid, vocabulary_size=VOCAB_SIZE, window_size=4) for wid in wids]\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "\n",
        "#view sample skip-grams\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "for i in range(len(pairs)):\n",
        "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "          idx2word[pairs[i][0]], pairs[i][0], \n",
        "          idx2word[pairs[i][1]], pairs[i][1], \n",
        "          labels[i]))"
      ],
      "metadata": {
        "id": "4zC4c4lIJWxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbabe984-35e0-497c-e681-411cf7d1f0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(austen (3), sensibility (1)) -> 1\n",
            "(sensibility (1), jane (2)) -> 1\n",
            "(sensibility (1), estrangement (10224)) -> 0\n",
            "(austen (3), moonlight (1663)) -> 0\n",
            "(austen (3), jane (2)) -> 1\n",
            "(sensibility (1), intruder (6100)) -> 0\n",
            "(jane (2), steele (3544)) -> 0\n",
            "(sensibility (1), austen (3)) -> 1\n",
            "(austen (3), pages (6015)) -> 0\n",
            "(jane (2), austen (3)) -> 1\n",
            "(jane (2), inure (9371)) -> 0\n",
            "(jane (2), sensibility (1)) -> 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDyy65QlTyf2"
      },
      "source": [
        "**Sanity Check:**\n",
        "\n",
        "To view the skip_grams for the first sentence in the training data, run the line of code that follows. The output should look like:\n",
        "\n",
        "(austen (3), sensibility (1)) -> 1\n",
        "\n",
        "(austen (3), jane (2)) -> 1\n",
        "\n",
        "(jane (2), sensibility (1)) -> 1\n",
        "\n",
        "(jane (2), walked (2639)) -> 0\n",
        "\n",
        "(jane (2), partridge (7948)) -> 0\n",
        "\n",
        "(sensibility (1), austen (3)) -> 1\n",
        "\n",
        "(sensibility (1), beneficial (5587)) -> 0\n",
        "\n",
        "(sensibility (1), jane (2)) -> 1\n",
        "\n",
        "(sensibility (1), dreamt (8308)) -> 0\n",
        "\n",
        "(austen (3), perception (6543)) -> 0\n",
        "\n",
        "(jane (2), austen (3)) -> 1\n",
        "\n",
        "(austen (3), imposing (8622)) -> 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMQJAolZWVLx"
      },
      "source": [
        "### **5. Building the Skip-gram Neural Network Architecture**\n",
        "\n",
        "In this section we would be building the skip-gram neural network architecture using the Keras Functional API and the Sequential model introduced in the previous lab. https://keras.io/getting-started/functional-api-guide/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fi0JmuY-Tlsp"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dot, Input\n",
        "from keras.layers.core import Dense, Reshape\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Model\n",
        "from keras.utils.vis_utils import plot_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22d8UIg-cBOc"
      },
      "source": [
        "The skip-gram model is two input one output feedforward neural network with one hidden layer and this will be built over a series of steps.\n",
        "\n",
        "####**A. The first step is to initialize and transform the first input using the following lines of code:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggqykF7WUyGP"
      },
      "outputs": [],
      "source": [
        "# The input is an array of target indices e.g. [2, 45, 7, 23,...9]\n",
        "target_word = Input((1,), dtype='int32')\n",
        "\n",
        "\n",
        "# feed the words into the model using the Keras <Embedding> layer. This is the hidden layer \n",
        "# from whose weights we will get the word embeddings.\n",
        "target_embedding = Embedding(VOCAB_SIZE, EMBED_SIZE, name='target_embed_layer',\n",
        "                        \tembeddings_initializer='glorot_uniform',\n",
        "                         \tinput_length=1)(target_word)\n",
        "\n",
        "\n",
        "# at this point, the input would of the shape (num_inputs x 1 x embed_size) and has to be flattened \n",
        "# or reshaped into a (num_inputs x embed_size) tensor.\n",
        "target_input = Reshape((EMBED_SIZE, ))(target_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXn_SVjHckwL"
      },
      "source": [
        "####**B. Write similar code for the ‘context_word’ input.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkCyRztYcMsp"
      },
      "outputs": [],
      "source": [
        "# your code for the context_word goes here\n",
        "# The input is an array of target indices e.g. [2, 45, 7, 23,...9]\n",
        "context_word = Input((1,), dtype='int32')\n",
        "\n",
        "\n",
        "# feed the words into the model using the Keras <Embedding> layer. This is the hidden layer \n",
        "# from whose weights we will get the word embeddings.\n",
        "context_embedding = Embedding(VOCAB_SIZE, EMBED_SIZE, name='context_embed_layer',\n",
        "                        \tembeddings_initializer='glorot_uniform',\n",
        "                         \tinput_length=1)(context_word)\n",
        "\n",
        "\n",
        "# at this point, the input would of the shape (num_inputs x 1 x embed_size) and has to be flattened \n",
        "# or reshaped into a (num_inputs x embed_size) tensor.\n",
        "context_input = Reshape((EMBED_SIZE, ))(context_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btNWVWRqdBJV"
      },
      "source": [
        "####**C. Merge the inputs.**\n",
        "\n",
        "Recall, each training instance is a (target_word, context_word) combination. Since we are trying to learn the degree of closeness between the two words, the model will compute the cosine distance between the two inputs using the <Dot> layer. https://keras.io/layers/merge/, hence fusing the two inputs into one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WL0_IgA0cYBx"
      },
      "outputs": [],
      "source": [
        "merged_inputs = Dot(axes=-1, normalize=False)([target_input, context_input])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQEsxQ6We1Ob"
      },
      "source": [
        "####**D. The Output Layer**\n",
        "\n",
        "Pass the merged inputs (now a vector with a single number the cosine distance between the two input vectors for each word) into a sigmoid activated neuron. The output of this layer is the output of the model.\n",
        "\n",
        "**Hint**: Use the <Dense> layer ( https://keras.io/layers/core/ ), with a ‘sigmoid’ activation function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# your code for the output layer goes here\n",
        "import tensorflow as tf\n",
        "out_layer = tf.keras.layers.Dense(1, activation='sigmoid')(merged_inputs)"
      ],
      "metadata": {
        "id": "nM0jvCDtaL20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQk1Yj8wfXrF"
      },
      "source": [
        "####**E. Initialize the model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHRICxY7cdBB"
      },
      "outputs": [],
      "source": [
        "# label is the output of step D.\n",
        "model = Model(inputs=[target_word, context_word], outputs=[out_layer])  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdbRI0TGVZIt"
      },
      "source": [
        "####**F. Compile the model using the <model.compile> command.** Use Loss = ‘mean_squared_error’, optimizer = ‘rmsprop’."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LtWvavefueu"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC7IShG_fczC"
      },
      "source": [
        "**Sanity check:**\n",
        "\n",
        "Visualize the model and the model summary by running the following lines of code. \n",
        "view the model summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ribodvykcqTP",
        "outputId": "1401d970-633a-4d67-cc64-ed24ece22ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " target_embed_layer (Embedding)  (None, 1, 100)      1028500     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " context_embed_layer (Embedding  (None, 1, 100)      1028500     ['input_2[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 100)          0           ['target_embed_layer[0][0]']     \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)            (None, 100)          0           ['context_embed_layer[0][0]']    \n",
            "                                                                                                  \n",
            " dot (Dot)                      (None, 1)            0           ['reshape[0][0]',                \n",
            "                                                                  'reshape_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1)            2           ['dot[0][0]']                    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,057,002\n",
            "Trainable params: 2,057,002\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VVPnyFhgVr9"
      },
      "source": [
        "####**G. Plot the model using ```vis utils```.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils import vis_utils\n",
        "SVG(vis_utils.model_to_dot(model, show_shapes=True, show_layer_names=True, dpi=60).create(prog='dot', format='svg'))  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "xu9JrhY4mrND",
        "outputId": "131a2cd8-53b8-421a-a74a-a1b6610b5899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"322pt\" viewBox=\"0.00 0.00 758.50 387.00\" width=\"632pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 383)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-383 754.5,-383 754.5,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139868074757200 -->\n<g class=\"node\" id=\"node1\">\n<title>139868074757200</title>\n<polygon fill=\"none\" points=\"30.5,-332.5 30.5,-378.5 330.5,-378.5 330.5,-332.5 30.5,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"70.5\" y=\"-363.3\">input_1</text>\n<polyline fill=\"none\" points=\"30.5,-355.5 110.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"70.5\" y=\"-340.3\">InputLayer</text>\n<polyline fill=\"none\" points=\"110.5,-332.5 110.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"139.5\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"110.5,-355.5 168.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"139.5\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"168.5,-332.5 168.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"209\" y=\"-351.8\">[(None, 1)]</text>\n<polyline fill=\"none\" points=\"249.5,-332.5 249.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"290\" y=\"-351.8\">[(None, 1)]</text>\n</g>\n<!-- 139869899868752 -->\n<g class=\"node\" id=\"node3\">\n<title>139869899868752</title>\n<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 361,-295.5 361,-249.5 0,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"64.5\" y=\"-280.3\">target_embed_layer</text>\n<polyline fill=\"none\" points=\"0,-272.5 129,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"64.5\" y=\"-257.3\">Embedding</text>\n<polyline fill=\"none\" points=\"129,-249.5 129,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"158\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"129,-272.5 187,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"158\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"187,-249.5 187,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223\" y=\"-268.8\">(None, 1)</text>\n<polyline fill=\"none\" points=\"259,-249.5 259,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"310\" y=\"-268.8\">(None, 1, 100)</text>\n</g>\n<!-- 139868074757200&#45;&gt;139869899868752 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139868074757200-&gt;139869899868752</title>\n<path d=\"M180.5,-332.3799C180.5,-324.1745 180.5,-314.7679 180.5,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"184.0001,-305.784 180.5,-295.784 177.0001,-305.784 184.0001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139869900359888 -->\n<g class=\"node\" id=\"node2\">\n<title>139869900359888</title>\n<polygon fill=\"none\" points=\"414.5,-332.5 414.5,-378.5 714.5,-378.5 714.5,-332.5 414.5,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"454.5\" y=\"-363.3\">input_2</text>\n<polyline fill=\"none\" points=\"414.5,-355.5 494.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"454.5\" y=\"-340.3\">InputLayer</text>\n<polyline fill=\"none\" points=\"494.5,-332.5 494.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"523.5\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"494.5,-355.5 552.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"523.5\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"552.5,-332.5 552.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"593\" y=\"-351.8\">[(None, 1)]</text>\n<polyline fill=\"none\" points=\"633.5,-332.5 633.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"674\" y=\"-351.8\">[(None, 1)]</text>\n</g>\n<!-- 139868074762896 -->\n<g class=\"node\" id=\"node4\">\n<title>139868074762896</title>\n<polygon fill=\"none\" points=\"378.5,-249.5 378.5,-295.5 750.5,-295.5 750.5,-249.5 378.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"448.5\" y=\"-280.3\">context_embed_layer</text>\n<polyline fill=\"none\" points=\"378.5,-272.5 518.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"448.5\" y=\"-257.3\">Embedding</text>\n<polyline fill=\"none\" points=\"518.5,-249.5 518.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"547.5\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"518.5,-272.5 576.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"547.5\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"576.5,-249.5 576.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"612.5\" y=\"-268.8\">(None, 1)</text>\n<polyline fill=\"none\" points=\"648.5,-249.5 648.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"699.5\" y=\"-268.8\">(None, 1, 100)</text>\n</g>\n<!-- 139869900359888&#45;&gt;139868074762896 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139869900359888-&gt;139868074762896</title>\n<path d=\"M564.5,-332.3799C564.5,-324.1745 564.5,-314.7679 564.5,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"568.0001,-305.784 564.5,-295.784 561.0001,-305.784 568.0001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139867776638928 -->\n<g class=\"node\" id=\"node5\">\n<title>139867776638928</title>\n<polygon fill=\"none\" points=\"48.5,-166.5 48.5,-212.5 360.5,-212.5 360.5,-166.5 48.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81\" y=\"-197.3\">reshape</text>\n<polyline fill=\"none\" points=\"48.5,-189.5 113.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81\" y=\"-174.3\">Reshape</text>\n<polyline fill=\"none\" points=\"113.5,-166.5 113.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"142.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"113.5,-189.5 171.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"142.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"171.5,-166.5 171.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222.5\" y=\"-185.8\">(None, 1, 100)</text>\n<polyline fill=\"none\" points=\"273.5,-166.5 273.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"317\" y=\"-185.8\">(None, 100)</text>\n</g>\n<!-- 139869899868752&#45;&gt;139867776638928 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139869899868752-&gt;139867776638928</title>\n<path d=\"M187.1853,-249.3799C189.5838,-241.0854 192.3371,-231.5633 194.9321,-222.5889\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"198.3517,-223.3627 197.7673,-212.784 191.6272,-221.4182 198.3517,-223.3627\" stroke=\"#000000\"/>\n</g>\n<!-- 139867767822224 -->\n<g class=\"node\" id=\"node6\">\n<title>139867767822224</title>\n<polygon fill=\"none\" points=\"391,-166.5 391,-212.5 712,-212.5 712,-166.5 391,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"428\" y=\"-197.3\">reshape_1</text>\n<polyline fill=\"none\" points=\"391,-189.5 465,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"428\" y=\"-174.3\">Reshape</text>\n<polyline fill=\"none\" points=\"465,-166.5 465,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"494\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"465,-189.5 523,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"494\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"523,-166.5 523,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"574\" y=\"-185.8\">(None, 1, 100)</text>\n<polyline fill=\"none\" points=\"625,-166.5 625,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"668.5\" y=\"-185.8\">(None, 100)</text>\n</g>\n<!-- 139868074762896&#45;&gt;139867767822224 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139868074762896-&gt;139867767822224</title>\n<path d=\"M560.8788,-249.3799C559.5936,-241.1745 558.1203,-231.7679 556.728,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"560.1522,-222.1219 555.1469,-212.784 553.2365,-223.2052 560.1522,-222.1219\" stroke=\"#000000\"/>\n</g>\n<!-- 139867708447632 -->\n<g class=\"node\" id=\"node7\">\n<title>139867708447632</title>\n<polygon fill=\"none\" points=\"200.5,-83.5 200.5,-129.5 542.5,-129.5 542.5,-83.5 200.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"219.5\" y=\"-114.3\">dot</text>\n<polyline fill=\"none\" points=\"200.5,-106.5 238.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"219.5\" y=\"-91.3\">Dot</text>\n<polyline fill=\"none\" points=\"238.5,-83.5 238.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"238.5,-106.5 296.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"296.5,-83.5 296.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"383.5\" y=\"-102.8\">[(None, 100), (None, 100)]</text>\n<polyline fill=\"none\" points=\"470.5,-83.5 470.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"506.5\" y=\"-102.8\">(None, 1)</text>\n</g>\n<!-- 139867776638928&#45;&gt;139867708447632 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139867776638928-&gt;139867708447632</title>\n<path d=\"M251.0188,-166.3799C271.1117,-156.3936 294.7848,-144.6279 315.8043,-134.1811\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"317.5441,-137.2249 324.9413,-129.6399 314.4286,-130.9564 317.5441,-137.2249\" stroke=\"#000000\"/>\n</g>\n<!-- 139867767822224&#45;&gt;139867708447632 -->\n<g class=\"edge\" id=\"edge6\">\n<title>139867767822224-&gt;139867708447632</title>\n<path d=\"M501.36,-166.3799C479.5087,-156.304 453.729,-144.4167 430.9224,-133.9003\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"432.2297,-130.649 421.683,-129.6399 429.2985,-137.0058 432.2297,-130.649\" stroke=\"#000000\"/>\n</g>\n<!-- 139869900346640 -->\n<g class=\"node\" id=\"node8\">\n<title>139869900346640</title>\n<polygon fill=\"none\" points=\"244.5,-.5 244.5,-46.5 498.5,-46.5 498.5,-.5 244.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"270.5\" y=\"-31.3\">dense</text>\n<polyline fill=\"none\" points=\"244.5,-23.5 296.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"270.5\" y=\"-8.3\">Dense</text>\n<polyline fill=\"none\" points=\"296.5,-.5 296.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"325.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"296.5,-23.5 354.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"325.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"354.5,-.5 354.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"390.5\" y=\"-19.8\">(None, 1)</text>\n<polyline fill=\"none\" points=\"426.5,-.5 426.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"462.5\" y=\"-19.8\">(None, 1)</text>\n</g>\n<!-- 139867708447632&#45;&gt;139869900346640 -->\n<g class=\"edge\" id=\"edge7\">\n<title>139867708447632-&gt;139869900346640</title>\n<path d=\"M371.5,-83.3799C371.5,-75.1745 371.5,-65.7679 371.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"375.0001,-56.784 371.5,-46.784 368.0001,-56.784 375.0001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbOcZD6igpxy"
      },
      "source": [
        "###**6.Training the Model**\n",
        "\n",
        "Run the following block of code to train the model for 5 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(word2idx) \n",
        "VOCAB_SIZE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4pMX1vVhBoJ",
        "outputId": "4c032c7b-e6fb-4bfc-ded3-66426a953886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10285"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_SIZE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnvUs0NihDoW",
        "outputId": "ad9dd3b5-f280-46c5-d2ee-c937500b479a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmQN4r52gmLR",
        "outputId": "c55fc39b-2a4e-4ce4-e999-ccdd8e9f8424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed all 12715 sentences\n",
            "Epoch: 1 Loss: 0.7499145418405533 \n",
            "\n",
            "Processed all 12715 sentences\n",
            "Epoch: 2 Loss: 0.7473815083503723 \n",
            "\n",
            "Processed all 12715 sentences\n",
            "Epoch: 3 Loss: 0.7450541406869888 \n",
            "\n",
            "Processed all 12715 sentences\n",
            "Epoch: 4 Loss: 0.7424442023038864 \n",
            "\n",
            "Processed all 12715 sentences\n",
            "Epoch: 5 Loss: 0.7394593507051468 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    epoch_loss = 0\n",
        "    for i, sent_examples in enumerate(skip_grams):\n",
        "        #print(sent_examples)\n",
        "        target_wds = np.array([pair[0] for pair in sent_examples[0]], dtype='int32')\n",
        "        context_wds = np.array([pair[1] for pair in sent_examples[0]], dtype='int32')\n",
        "        labels = np.array(sent_examples[1], dtype='int32')\n",
        "        X = [target_wds, context_wds]\n",
        "        Y = labels\n",
        "        if i % 5000 == 0: \n",
        "        \t#print('Processed %d sentences' %i)\n",
        "          epoch_loss += model.train_on_batch(X, Y)\n",
        "    print('Processed all %d sentences' %i)\n",
        "    print('Epoch:', epoch, 'Loss:', epoch_loss, '\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj6BiOKfiI7R"
      },
      "source": [
        "The training takes about 10 minutes to run.\n",
        "\n",
        "<br>\n",
        "\n",
        "In the introduction, we outlined two approaches to training using the skipgram architecture. In this tutorial, we implemented the negative sampling training approach. While waiting for the training to complete, read this article http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/. It contains the skipgram model with softmax training. \n",
        "\n",
        "<br>\n",
        "\n",
        "After reading the article, answer the following questions:  \n",
        "●\tWhat would the inputs and outputs to the model be?\n",
        "\n",
        "●\tHow would you use the Keras framework to create this architecture?\n",
        "\n",
        "●\tWhat are the reasons this training approach is \n",
        "considered inefficient?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwz_8kTZ6UVF"
      },
      "source": [
        "###**7. Getting the Word Embeddings**\n",
        "\n",
        "The word embeddings are the weights of the target word embedding layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiQTO6VgcN90",
        "outputId": "0fc196dc-e3d9-48a5-95f7-90a435a6a5dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10285, 100)\n"
          ]
        }
      ],
      "source": [
        "word_embeddings = model.get_layer('target_embed_layer').get_weights()[0] \n",
        "\n",
        "# should return (VOCAB_SIZE, EMBED_SIZE)\n",
        "print(word_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33-QRJFv6v5t"
      },
      "source": [
        "Print out a few words and their embeddings using the next block of code. Your output may not be exactly as above but the command should print 10 words and their respective vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HffZwwLUcXb1",
        "outputId": "e69df72a-d8d9-4ff1-b678-99b120c7f3ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   0         1         2         3         4         5   \\\n",
            "sense        0.011421 -0.014969  0.010833 -0.014109 -0.011458 -0.022278   \n",
            "sensibility  0.026893  0.019133 -0.010789  0.005050 -0.037341 -0.020763   \n",
            "jane        -0.025423  0.016799 -0.006891 -0.036728 -0.027626 -0.021483   \n",
            "austen      -0.005710 -0.023441 -0.006604 -0.010186 -0.019812 -0.000386   \n",
            "family      -0.020713 -0.020265  0.009221  0.012659  0.001446 -0.009731   \n",
            "dashwood    -0.011390  0.011597  0.004675 -0.010362  0.002304  0.001367   \n",
            "long        -0.017342  0.012111 -0.015062  0.015229  0.013776 -0.000436   \n",
            "settled     -0.004241 -0.015434 -0.020345 -0.008194 -0.015890  0.003723   \n",
            "sussex      -0.009207  0.005251 -0.017988  0.000976  0.021763 -0.001083   \n",
            "estate       0.003426  0.008277  0.009064  0.001862  0.017115 -0.007849   \n",
            "\n",
            "                   6         7         8         9   ...        90        91  \\\n",
            "sense       -0.016098 -0.004706  0.022191  0.009888  ...  0.015120  0.007817   \n",
            "sensibility -0.021511 -0.010876 -0.021492 -0.021791  ...  0.026239 -0.029278   \n",
            "jane        -0.011140  0.029163  0.000469  0.011607  ... -0.027268 -0.025377   \n",
            "austen       0.023650  0.016155  0.026603 -0.025344  ...  0.004194 -0.003936   \n",
            "family       0.023887  0.004627 -0.007519 -0.021781  ... -0.020493  0.022704   \n",
            "dashwood    -0.023812 -0.010837 -0.010242 -0.021865  ...  0.005448  0.006056   \n",
            "long         0.019080  0.003322  0.020337 -0.010475  ...  0.013311 -0.007459   \n",
            "settled      0.001590  0.007797 -0.010429  0.008049  ... -0.008180 -0.011171   \n",
            "sussex       0.000832  0.015407  0.013137  0.001244  ...  0.002441  0.020551   \n",
            "estate       0.014170 -0.022671  0.004014  0.003264  ...  0.004591 -0.022537   \n",
            "\n",
            "                   92        93        94        95        96        97  \\\n",
            "sense       -0.000025 -0.007100  0.020506 -0.002781 -0.020601 -0.010991   \n",
            "sensibility  0.019793  0.016487 -0.002518 -0.028056  0.030725  0.012661   \n",
            "jane        -0.013829  0.019149 -0.034874  0.021220 -0.031124  0.012931   \n",
            "austen      -0.031051  0.029627 -0.026625  0.009824  0.026641 -0.015338   \n",
            "family       0.011963 -0.002346 -0.020900  0.022921  0.001527 -0.014461   \n",
            "dashwood     0.002269 -0.014208  0.003064  0.019958  0.017818 -0.004027   \n",
            "long         0.015315  0.005988 -0.005215 -0.006880  0.009919 -0.004892   \n",
            "settled     -0.000041 -0.019852 -0.007536 -0.002754 -0.018901  0.010092   \n",
            "sussex      -0.002566  0.016856  0.004802  0.015068  0.013016 -0.004050   \n",
            "estate       0.000188  0.003197  0.014845  0.004400  0.001815  0.001969   \n",
            "\n",
            "                   98        99  \n",
            "sense        0.014706 -0.001632  \n",
            "sensibility -0.005744 -0.036705  \n",
            "jane        -0.008172  0.020275  \n",
            "austen       0.014900  0.014343  \n",
            "family      -0.018600  0.021308  \n",
            "dashwood     0.017823 -0.010482  \n",
            "long        -0.012376 -0.001740  \n",
            "settled     -0.024030 -0.004367  \n",
            "sussex      -0.012983 -0.018987  \n",
            "estate       0.015431  0.018490  \n",
            "\n",
            "[10 rows x 100 columns]\n"
          ]
        }
      ],
      "source": [
        "from pandas import DataFrame\n",
        "\n",
        "print(DataFrame(word_embeddings, index=idx2word.values()).head(10))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_df=DataFrame(word_embeddings, index=idx2word.values())\n",
        "word_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "d1XOSz59_6n0",
        "outputId": "02ef979e-aab7-4d23-faad-9e95fcc08c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   0         1         2         3         4         5   \\\n",
              "sense        0.011421 -0.014969  0.010833 -0.014109 -0.011458 -0.022278   \n",
              "sensibility  0.026893  0.019133 -0.010789  0.005050 -0.037341 -0.020763   \n",
              "jane        -0.025423  0.016799 -0.006891 -0.036728 -0.027626 -0.021483   \n",
              "austen      -0.005710 -0.023441 -0.006604 -0.010186 -0.019812 -0.000386   \n",
              "family      -0.020713 -0.020265  0.009221  0.012659  0.001446 -0.009731   \n",
              "...               ...       ...       ...       ...       ...       ...   \n",
              "depriving    0.005735 -0.000271  0.016626  0.015267 -0.003449 -0.010014   \n",
              "requited     0.000546  0.011585  0.009229 -0.009010  0.002357  0.006917   \n",
              "supplies     0.023791  0.000309 -0.007642  0.006409 -0.006066 -0.003926   \n",
              "accessions  -0.015418  0.002031 -0.019518 -0.021644  0.011509 -0.015024   \n",
              "national    -0.003307  0.006839  0.023635  0.016531 -0.021248  0.015303   \n",
              "\n",
              "                   6         7         8         9   ...        90        91  \\\n",
              "sense       -0.016098 -0.004706  0.022191  0.009888  ...  0.015120  0.007817   \n",
              "sensibility -0.021511 -0.010876 -0.021492 -0.021791  ...  0.026239 -0.029278   \n",
              "jane        -0.011140  0.029163  0.000469  0.011607  ... -0.027268 -0.025377   \n",
              "austen       0.023650  0.016155  0.026603 -0.025344  ...  0.004194 -0.003936   \n",
              "family       0.023887  0.004627 -0.007519 -0.021781  ... -0.020493  0.022704   \n",
              "...               ...       ...       ...       ...  ...       ...       ...   \n",
              "depriving    0.010958 -0.015334  0.000005  0.021348  ...  0.016769  0.017322   \n",
              "requited    -0.009854 -0.000806 -0.008544 -0.006800  ...  0.019124  0.009250   \n",
              "supplies     0.013844  0.009408  0.021634 -0.004767  ... -0.020727 -0.021149   \n",
              "accessions  -0.011472  0.015115 -0.022306  0.006589  ... -0.012950 -0.015341   \n",
              "national    -0.018532 -0.007529 -0.013235 -0.012244  ...  0.012245  0.009625   \n",
              "\n",
              "                   92        93        94        95        96        97  \\\n",
              "sense       -0.000025 -0.007100  0.020506 -0.002781 -0.020601 -0.010991   \n",
              "sensibility  0.019793  0.016487 -0.002518 -0.028056  0.030725  0.012661   \n",
              "jane        -0.013829  0.019149 -0.034874  0.021220 -0.031124  0.012931   \n",
              "austen      -0.031051  0.029627 -0.026625  0.009824  0.026641 -0.015338   \n",
              "family       0.011963 -0.002346 -0.020900  0.022921  0.001527 -0.014461   \n",
              "...               ...       ...       ...       ...       ...       ...   \n",
              "depriving   -0.022789  0.007582 -0.014869 -0.006077  0.015942  0.018623   \n",
              "requited    -0.019691  0.008051  0.021526 -0.004634  0.001648  0.008214   \n",
              "supplies    -0.004513 -0.019734 -0.020098  0.003801  0.020931  0.006039   \n",
              "accessions  -0.002836 -0.020590  0.012014 -0.010508 -0.018682 -0.016913   \n",
              "national     0.017901 -0.016998  0.015799 -0.017933  0.013863  0.011131   \n",
              "\n",
              "                   98        99  \n",
              "sense        0.014706 -0.001632  \n",
              "sensibility -0.005744 -0.036705  \n",
              "jane        -0.008172  0.020275  \n",
              "austen       0.014900  0.014343  \n",
              "family      -0.018600  0.021308  \n",
              "...               ...       ...  \n",
              "depriving   -0.000392 -0.022566  \n",
              "requited    -0.008878  0.001802  \n",
              "supplies     0.018451 -0.015784  \n",
              "accessions  -0.002214  0.011073  \n",
              "national    -0.007157  0.008070  \n",
              "\n",
              "[10285 rows x 100 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5ba1b9a4-dc87-481f-9ef8-2e7015606073\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>sense</th>\n",
              "      <td>0.011421</td>\n",
              "      <td>-0.014969</td>\n",
              "      <td>0.010833</td>\n",
              "      <td>-0.014109</td>\n",
              "      <td>-0.011458</td>\n",
              "      <td>-0.022278</td>\n",
              "      <td>-0.016098</td>\n",
              "      <td>-0.004706</td>\n",
              "      <td>0.022191</td>\n",
              "      <td>0.009888</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015120</td>\n",
              "      <td>0.007817</td>\n",
              "      <td>-0.000025</td>\n",
              "      <td>-0.007100</td>\n",
              "      <td>0.020506</td>\n",
              "      <td>-0.002781</td>\n",
              "      <td>-0.020601</td>\n",
              "      <td>-0.010991</td>\n",
              "      <td>0.014706</td>\n",
              "      <td>-0.001632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sensibility</th>\n",
              "      <td>0.026893</td>\n",
              "      <td>0.019133</td>\n",
              "      <td>-0.010789</td>\n",
              "      <td>0.005050</td>\n",
              "      <td>-0.037341</td>\n",
              "      <td>-0.020763</td>\n",
              "      <td>-0.021511</td>\n",
              "      <td>-0.010876</td>\n",
              "      <td>-0.021492</td>\n",
              "      <td>-0.021791</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026239</td>\n",
              "      <td>-0.029278</td>\n",
              "      <td>0.019793</td>\n",
              "      <td>0.016487</td>\n",
              "      <td>-0.002518</td>\n",
              "      <td>-0.028056</td>\n",
              "      <td>0.030725</td>\n",
              "      <td>0.012661</td>\n",
              "      <td>-0.005744</td>\n",
              "      <td>-0.036705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jane</th>\n",
              "      <td>-0.025423</td>\n",
              "      <td>0.016799</td>\n",
              "      <td>-0.006891</td>\n",
              "      <td>-0.036728</td>\n",
              "      <td>-0.027626</td>\n",
              "      <td>-0.021483</td>\n",
              "      <td>-0.011140</td>\n",
              "      <td>0.029163</td>\n",
              "      <td>0.000469</td>\n",
              "      <td>0.011607</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027268</td>\n",
              "      <td>-0.025377</td>\n",
              "      <td>-0.013829</td>\n",
              "      <td>0.019149</td>\n",
              "      <td>-0.034874</td>\n",
              "      <td>0.021220</td>\n",
              "      <td>-0.031124</td>\n",
              "      <td>0.012931</td>\n",
              "      <td>-0.008172</td>\n",
              "      <td>0.020275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>austen</th>\n",
              "      <td>-0.005710</td>\n",
              "      <td>-0.023441</td>\n",
              "      <td>-0.006604</td>\n",
              "      <td>-0.010186</td>\n",
              "      <td>-0.019812</td>\n",
              "      <td>-0.000386</td>\n",
              "      <td>0.023650</td>\n",
              "      <td>0.016155</td>\n",
              "      <td>0.026603</td>\n",
              "      <td>-0.025344</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004194</td>\n",
              "      <td>-0.003936</td>\n",
              "      <td>-0.031051</td>\n",
              "      <td>0.029627</td>\n",
              "      <td>-0.026625</td>\n",
              "      <td>0.009824</td>\n",
              "      <td>0.026641</td>\n",
              "      <td>-0.015338</td>\n",
              "      <td>0.014900</td>\n",
              "      <td>0.014343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>family</th>\n",
              "      <td>-0.020713</td>\n",
              "      <td>-0.020265</td>\n",
              "      <td>0.009221</td>\n",
              "      <td>0.012659</td>\n",
              "      <td>0.001446</td>\n",
              "      <td>-0.009731</td>\n",
              "      <td>0.023887</td>\n",
              "      <td>0.004627</td>\n",
              "      <td>-0.007519</td>\n",
              "      <td>-0.021781</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020493</td>\n",
              "      <td>0.022704</td>\n",
              "      <td>0.011963</td>\n",
              "      <td>-0.002346</td>\n",
              "      <td>-0.020900</td>\n",
              "      <td>0.022921</td>\n",
              "      <td>0.001527</td>\n",
              "      <td>-0.014461</td>\n",
              "      <td>-0.018600</td>\n",
              "      <td>0.021308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>depriving</th>\n",
              "      <td>0.005735</td>\n",
              "      <td>-0.000271</td>\n",
              "      <td>0.016626</td>\n",
              "      <td>0.015267</td>\n",
              "      <td>-0.003449</td>\n",
              "      <td>-0.010014</td>\n",
              "      <td>0.010958</td>\n",
              "      <td>-0.015334</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.021348</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016769</td>\n",
              "      <td>0.017322</td>\n",
              "      <td>-0.022789</td>\n",
              "      <td>0.007582</td>\n",
              "      <td>-0.014869</td>\n",
              "      <td>-0.006077</td>\n",
              "      <td>0.015942</td>\n",
              "      <td>0.018623</td>\n",
              "      <td>-0.000392</td>\n",
              "      <td>-0.022566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>requited</th>\n",
              "      <td>0.000546</td>\n",
              "      <td>0.011585</td>\n",
              "      <td>0.009229</td>\n",
              "      <td>-0.009010</td>\n",
              "      <td>0.002357</td>\n",
              "      <td>0.006917</td>\n",
              "      <td>-0.009854</td>\n",
              "      <td>-0.000806</td>\n",
              "      <td>-0.008544</td>\n",
              "      <td>-0.006800</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019124</td>\n",
              "      <td>0.009250</td>\n",
              "      <td>-0.019691</td>\n",
              "      <td>0.008051</td>\n",
              "      <td>0.021526</td>\n",
              "      <td>-0.004634</td>\n",
              "      <td>0.001648</td>\n",
              "      <td>0.008214</td>\n",
              "      <td>-0.008878</td>\n",
              "      <td>0.001802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>supplies</th>\n",
              "      <td>0.023791</td>\n",
              "      <td>0.000309</td>\n",
              "      <td>-0.007642</td>\n",
              "      <td>0.006409</td>\n",
              "      <td>-0.006066</td>\n",
              "      <td>-0.003926</td>\n",
              "      <td>0.013844</td>\n",
              "      <td>0.009408</td>\n",
              "      <td>0.021634</td>\n",
              "      <td>-0.004767</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020727</td>\n",
              "      <td>-0.021149</td>\n",
              "      <td>-0.004513</td>\n",
              "      <td>-0.019734</td>\n",
              "      <td>-0.020098</td>\n",
              "      <td>0.003801</td>\n",
              "      <td>0.020931</td>\n",
              "      <td>0.006039</td>\n",
              "      <td>0.018451</td>\n",
              "      <td>-0.015784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accessions</th>\n",
              "      <td>-0.015418</td>\n",
              "      <td>0.002031</td>\n",
              "      <td>-0.019518</td>\n",
              "      <td>-0.021644</td>\n",
              "      <td>0.011509</td>\n",
              "      <td>-0.015024</td>\n",
              "      <td>-0.011472</td>\n",
              "      <td>0.015115</td>\n",
              "      <td>-0.022306</td>\n",
              "      <td>0.006589</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.012950</td>\n",
              "      <td>-0.015341</td>\n",
              "      <td>-0.002836</td>\n",
              "      <td>-0.020590</td>\n",
              "      <td>0.012014</td>\n",
              "      <td>-0.010508</td>\n",
              "      <td>-0.018682</td>\n",
              "      <td>-0.016913</td>\n",
              "      <td>-0.002214</td>\n",
              "      <td>0.011073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>national</th>\n",
              "      <td>-0.003307</td>\n",
              "      <td>0.006839</td>\n",
              "      <td>0.023635</td>\n",
              "      <td>0.016531</td>\n",
              "      <td>-0.021248</td>\n",
              "      <td>0.015303</td>\n",
              "      <td>-0.018532</td>\n",
              "      <td>-0.007529</td>\n",
              "      <td>-0.013235</td>\n",
              "      <td>-0.012244</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012245</td>\n",
              "      <td>0.009625</td>\n",
              "      <td>0.017901</td>\n",
              "      <td>-0.016998</td>\n",
              "      <td>0.015799</td>\n",
              "      <td>-0.017933</td>\n",
              "      <td>0.013863</td>\n",
              "      <td>0.011131</td>\n",
              "      <td>-0.007157</td>\n",
              "      <td>0.008070</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10285 rows × 100 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ba1b9a4-dc87-481f-9ef8-2e7015606073')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5ba1b9a4-dc87-481f-9ef8-2e7015606073 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5ba1b9a4-dc87-481f-9ef8-2e7015606073');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ki8NtrS7ezr"
      },
      "source": [
        "###**8.  Measuring Similarity Between Word Pairs**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbb-uoe666oV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd53fc3f-133c-4b7a-cf88-90202c4f40d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10285, 10285)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = cosine_similarity(word_embeddings)\n",
        "\n",
        "# should print(VOCAB_SIZE, VOCAB_SIZE)\n",
        "print(similarity_matrix.shape)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0fyfgvB90G5"
      },
      "source": [
        "###**9. Exploring and Visualizing your Word Embeddings using t-SNE**\n",
        "\n",
        "**A.\tGet the most similar words to the search items in the list below**\n",
        "\n",
        "search_terms = ['family', 'love', 'equality', 'wisdom', 'justice',  'humour', 'rejection']\n",
        "\n",
        "\n",
        "**Sanity check:**\n",
        "The similar words obtained would depend on your training but the above command should print a dictionary. Each key is a search term and each value is a list of the 5 words the model predicts to be most similar to the key word. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_terms = ['think', 'thought', 'mr', 'friend', 'love', 'disdain']\n",
        "\n",
        "similar_words = {search_term: [idx2word[idx] for\n",
        "                 idx in similarity_matrix[word2idx[search_term]-1].argsort()[1:6]+1]\n",
        "                  for search_term in search_terms}\n",
        "\n",
        "print(similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi_1bXQ31A4K",
        "outputId": "96ce2b08-2063-488f-e959-26ff72e18e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'think': ['uncommon', 'clearly', 'campbells', 'vision', 'shrubberies'], 'thought': ['control', 'passionate', 'umbrella', 'chained', 'presumption'], 'mr': ['formidable', 'dropped', 'reclaim', 'another', 'courted'], 'friend': ['interpret', 'destroyed', 'nature', 'feeling', 'relics'], 'love': ['mortified', 'made', 'confederacy', 'unfinished', 'commission'], 'disdain': ['relish', 'judgment', 'boys', 'wrist', 'cavil']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWbSfqYschRs",
        "outputId": "ed9a75a6-3960-473d-afbf-ac430e659e69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'think': ['assist', 'mean', 'possible', 'made', 'neglect'], 'thought': ['upon', 'honour', 'restriction', 'every', 'specimen'], 'mr': ['connexions', 'unexceptionable', 'gain', 'woodhouse', 'suffer'], 'friend': ['woodhouse', 'specimen', 'gain', 'subjects', 'excellent'], 'love': ['made', 'paid', 'offended', 'subjects', 'differed'], 'disdain': ['unprepared', 'confiding', 'splendidly', 'unkindness', 'extinguished']}\n"
          ]
        }
      ],
      "source": [
        "search_terms = ['think', 'thought', 'mr', 'friend', 'love', 'disdain']\n",
        "\n",
        "similar_words = {search_term: [idx2word[idx] for\n",
        "                 idx in similarity_matrix[word2idx[search_term]-1].argsort()[1:6]+1]\n",
        "                  for search_term in search_terms}\n",
        "\n",
        "print(similar_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I_X9W-vGXiZ"
      },
      "source": [
        "**B. Plot the words in the dictionary above using t-SN**E \n",
        "\n",
        "1.   List item\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "2.   List item\n",
        "\n",
        "https://lvdmaaten.github.io/tsne/ \n",
        "\n",
        "Plot 50 of the word embeddings using the code snippets below:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tsne = TSNE(perplexity=3, n_components=2, init='pca', n_iter=5000, method='exact')\n",
        "np.set_printoptions(suppress=True)\n",
        "plot_only = 50 \n",
        "\n",
        "T = tsne.fit_transform(word_embeddings[:plot_only, :])\n",
        "labels = [idx2word[i+1] for i in range(plot_only)]\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.scatter(T[:, 0], T[:, 1])\n",
        "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
        "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points', ha='right', va='bottom')                      \t                        \n"
      ],
      "metadata": {
        "id": "UT74_S0CYiEJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "outputId": "51805689-5ddf-48e0-c9e4-be5c6a2b278e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tS8vqkdYXDQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsCVTRUMP1qi"
      },
      "source": [
        "**10. Resources used**\n",
        "\n",
        "http://jalammar.github.io/illustrated-word2vec/\n",
        "https://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/ \n",
        "https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa \n",
        "https://adventuresinmachinelearning.com/word2vec-keras-tutorial/ \n",
        "https://www.tensorflow.org/tutorials/representation/word2vec#the_skip-gram_model \n",
        "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KyO2aPYi5jmk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "7001_2021_22_lab1_wordEmbeddings_without_answers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}